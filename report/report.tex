\documentclass[acmlarge]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{On the parallelization of a N-body simulation}

\author{Alice}
\email{alice@mail.com}
\affiliation{
  \institution{Universidade do Minho}
  \city{Braga}
  \country{Portugal}
}
\author{Bob}
\email{bob@mail.com}
\affiliation{
  \institution{Universidade do Minho}
  \city{Braga}
  \country{Portugal}
}
\author{Charlie}
\email{charlie@mail.com}
\affiliation{
  \institution{Universidade do Minho}
  \city{Braga}
  \country{Portugal}
}

\renewcommand{\shortauthors}{Alice et al.}


\begin{abstract}
  This report documents the parallelization techniques implemented on a N-body simulation. This was achieved through the OpenMP API. After carefully analyzing and tested different optimization locations, the resulting program had a xxx\% speedup compared to the original, sequential implementation. This result shows that physical, N-body simulations are an excellent target for parallel code optimizations. As even very simple optimization techniques are able to produce a huge speedup.
\end{abstract}

\keywords{Parallel, Parallelization, Code Optimization, Simulation, Physics Simulation, OpenMP}


\maketitle

\section{Introduction}

With the gradual decline of Moore's Law\cite{Moore_Law}, the time of swapping an older chip for a newer one and experiencing exponential speedups is coming to an end. This, in a sense, limits the computing power a certain problem can have to still be tractable. That is, if we were dealing with a single computing core on a machine.

Fortunately, that is not the case. Nowadays, even the simplest computers have multiple computing cores. Unfortunately, this parallelism is often misused or even neglected, making the device, essentially, a single-core computer.


As such, this demands a narrative shift to parallelism as design principle. In fact, this is precisely the direction in which hardware is going, with more cores instead of faster ones, as we are reaching the physical limits of silicon. Thus, performance gains are not dependent on how fast a given CPU is, but how the parallelism of a given CPU can be exploited.


This issue becomes quite significant when it comes to N-body problems. These problems are notorious for their computational complexity, this is due to the pairwise calculations between every body in the system. As such, this requires a $O(N^2)$ time complexity, meaning that even small increases in particle count could dramatically increase the runtime of such an algorithm. 
In real-world scenarios, these applications require millions, if not billions, of particles in order to simulate any interesting phenomena. Thus making serial implementations of these applications impractical. This necessitates parallelization for these kinds of problems. 
However, parallelism isn't a solution without any drawbacks, there are a lot of considerations when parallelizing any algorithm. Such as the synchronization overheads, load imbalances, cache coherence effects, and even, memory bandwidth limitations.
This article goes over the parallelization of a N-body simulation on multicore CPUs, focusing on memory behavior, scalability and algorithmic design choices.


\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
\endinput
