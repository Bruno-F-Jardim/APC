\documentclass[acmlarge]{acmart}
\usepackage{listings}
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

\begin{document}

\title{On the parallelization of a N-body simulation}

\author{Andr√© Mendes}
\email{andre@mail.com}
\affiliation{
  \institution{Universidade do Minho}
  \city{Braga}
  \country{Portugal}
}
\author{Bruno Jardim}
\email{bruno.f.jardim@inesctec.pt}
\affiliation{
  \institution{Universidade do Minho}
  \city{Braga}
  \country{Portugal}
}
\author{Roberto Figueiredo}
\email{roberto@mail.com}
\affiliation{
  \institution{Universidade do Minho}
  \city{Braga}
  \country{Portugal}
}

\renewcommand{\shortauthors}{Mendes et al.}


\begin{abstract}
  This report documents the parallelization techniques implemented on a N-body simulation. This was achieved through the OpenMP\cite{openmp_shared_memory_model} API. After carefully analyzing and tested different optimization locations, the resulting program had a xxx\% speedup compared to the original, sequential implementation. This result shows that physical, N-body simulations are an excellent target for parallel code optimizations. As even very simple optimization techniques are able to produce a huge speedup.
\end{abstract}

\keywords{Parallel, Parallelization, Code Optimization, Simulation, Physics Simulation, OpenMP}


\maketitle

\section{Introduction}

With the gradual decline of Moore's Law\cite{Moore_Law}, the time of swapping an older chip for a newer one and experiencing exponential speedups is coming to an end. This, in a sense, limits the computing power a certain problem can have to still be tractable. That is, if we were dealing with a single computing core on a machine.

Fortunately, that is not the case. Nowadays, even the simplest computers have multiple computing cores. Unfortunately, this parallelism is often misused or even neglected, making the device, essentially, a single-core computer.


As such, this demands a narrative shift to parallelism as design principle. In fact, this is precisely the direction in which hardware is going, with more cores instead of faster ones, as we are reaching the physical limits of silicon. Thus, performance gains are not dependent on how fast a given CPU is, but how the parallelism of a given CPU can be exploited.


This issue becomes quite significant when it comes to N-body problems. These problems are notorious for their computational complexity, this is due to the pairwise calculations between every body in the system. As such, this requires a $O(N^2)$ time complexity, meaning that even small increases in particle count could dramatically increase the runtime of such an algorithm. 
In real-world scenarios, these applications require millions, if not billions, of particles in order to simulate any interesting phenomena. Thus making serial implementations of these applications impractical. This necessitates parallelization for these kinds of problems. 
However, parallelism isn't a solution without any drawbacks, there are a lot of considerations when parallelizing any algorithm. Such as the synchronization overheads, load imbalances, cache coherence effects, and even, memory bandwidth limitations.
This article goes over the parallelization of a N-body simulation on multicore CPUs, focusing on memory behavior, scalability and algorithmic design choices.

\section{Background}
\subsection{Overview of the N-body Problem}
The N-body problem aims to simulate the time evolution of a given system with N particles, where all particles affect one another.
Each time step necessitates the computation of the net forces that act on every single particle based on the states of all others and then updating their position and velocity.
This results in an $O(N^2)$ complexity on every time step. Making the problem increasingly more expensive as $N$ grows.

Due to this design the N-body problem is a well-suited candidate for parallelization. This is further reinforced by the fact that the force computations on individual particles are independent within that time step. However, most implementations on multicore CPUs are constrained by memory bottlenecks, cache coherence effects and even synchronization overheads.
Thus, achieving linear speedups is not as trivial as it might seem.

Performance is heavily influenced by memory access patterns along with the in-memory representation of the given datastructures.
As they can significantly improve cache utilization and throughput.
All of these characteristics make the N-body problem a very useful benchmark for parallel performance on modern CPU architectures.

\subsection{Parallel Computing on Multicore CPUs with OpenMP} 
OpenMP\cite{openmp_shared_memory_model} is a commonly used programming model for shared-memory parallelism on multicore CPUs, providing a directive-based approach for defining parallel regions, work sharing, and synchronization. It allows existing sequential code to be parallelized incrementally, while maintaining a single shared address space. However, this abstraction does not remove the underlying costs associated with thread management, synchronization, and memory access, and as such performance still depends heavily on implementation details.

Parallel execution in OpenMP is most often achieved by distributing loop iterations or tasks across a team of threads. This maps well to data-parallel workloads such as N-body force calculations. However, scalability is influenced by scheduling choices such as the granularity of parallel work, and the amount of implicit synchronization introduced by constructs like barriers and reductions. Thus, suboptimal scheduling or excessive synchronization can limit performance improvements.

Memory behavior remains a central concern in OpenMP programs. Threads operate on cache-coherent shared memory, and frequent access to shared data can increase cache coherence traffic. This can lead to issues such as false sharing and reduced cache efficiency, which thus negatively affect scalability. As such, achieving good performance with OpenMP requires careful consideration of data layout, access patterns, and synchronization placement, in addition to exposing sufficient parallelism.

\section{Sequential Implementation of the N-Body Simulation}

The sequential implementation of the N-body simulation follows a straightforward time-stepping approach based on direct pairwise force computation. At each simulation step, the physics engine computes gravitational accelerations for all bodies and then updates their velocities and positions using explicit numerical integration. This implementation serves as the performance and correctness baseline against which parallel versions are evaluated.

The core computation is performed in the \texttt{integrateStep} function, which operates on a vector of bodies. For a system of $N$ bodies, the function first allocates an auxiliary array to store the acceleration vectors for each body. The accelerations are initialized to zero at the beginning of each step, ensuring that force accumulation from the previous iteration does not persist. As such, force computation and state updates are cleanly separated.

Gravitational forces are computed using a double-nested loop that iterates over all pairs of bodies. For each body $i$, the contribution from every other body $j$ is accumulated, excluding self-interactions. The force calculation is based on the classical inverse-square law, with a small softening term added to the distance computation to avoid numerical instability when bodies are very close. This approach results in a computational complexity of $O(N^2)$ per simulation step, which quickly becomes the dominant cost as the number of bodies increases.

Once all accelerations have been computed, a second loop updates the velocities and positions of each body using a simple explicit integration scheme. Velocities are updated based on the computed accelerations and the time step size, and positions are then advanced using the updated velocities. This integration method is computationally inexpensive but conditionally stable, and thus appropriate for short time steps and as a baseline reference.

Overall, the sequential implementation emphasizes clarity and correctness rather than performance optimization. It makes no attempt to reduce asymptotic complexity or exploit hardware parallelism. However, its simple structure and deterministic execution order make it well suited as a reference point for evaluating the effects of parallelization, memory behavior, and scalability in subsequent implementations.

\section{ILP Optimizations}


The first step when optimizing the sequential implementation is exploring the inherent capabilities of modern CPUs to execute multiple instructions concurrently. Even without introducing thread-level parallelism, the baseline N-body implementation exposes opportunities for improved instruction-level parallelism through careful ordering of arithmetic operations and memory accesses. As such, changes such as reducing control dependencies, increasing the number of independent operations, and improving data locality can significantly increase single-core performance. These optimizations preserve the sequential execution model while providing a more efficient foundation for subsequent shared-memory parallelization.

\begin{lstlisting}[language=C++]
void PhysicsEngine::integrateStep(SoABodies& bodies, double dt) {
    const double G = 6.67430e-11;
    size_t n = bodies.size();

    // Reset accelerations
    std::fill(bodies.acc_x.begin(), bodies.acc_x.end(), 0.0);
    std::fill(bodies.acc_y.begin(), bodies.acc_y.end(), 0.0);
    std::fill(bodies.acc_z.begin(), bodies.acc_z.end(), 0.0);

    for (size_t i = 0; i < n; ++i) {
        double ax = 0.0, ay = 0.0, az = 0.0;

        double px = bodies.pos_x[i];
        double py = bodies.pos_y[i];
        double pz = bodies.pos_z[i];

        for (size_t j = 0; j < n; ++j) {
            // if (i == j) continue; removed to optimize performance
            
            double rx = bodies.pos_x[j] - px;
            double ry = bodies.pos_y[j] - py;
            double rz = bodies.pos_z[j] - pz;
            
            double distSq = rx * rx + ry * ry + rz * rz + 1e-9;
            double inv_dist = 1.0 / std::sqrt(distSq);
            double inv_dist3 = inv_dist * inv_dist * inv_dist;
            double factor = G * bodies.mass[j] * inv_dist3;
            ax += factor * rx;
            ay += factor * ry;
            az += factor * rz;
            
        }
        bodies.acc_x[i] += ax;
        bodies.acc_y[i] += ay;
        bodies.acc_z[i] += az;
    }

    for (size_t i = 0; i < n; ++i) {
        bodies.vel_x[i] += bodies.acc_x[i] * dt;
        bodies.vel_y[i] += bodies.acc_y[i] * dt;
        bodies.vel_z[i] += bodies.acc_z[i] * dt;
        
        bodies.pos_x[i] += bodies.vel_x[i] * dt;
        bodies.pos_y[i] += bodies.vel_y[i] * dt;
        bodies.pos_z[i] += bodies.vel_z[i] * dt;
    }
}
\end{lstlisting}

The implementation exposes a significant amount of instruction-level parallelism through careful ordering of arithmetic operations and data access patterns. In the inner force computation loop, independent floating-point operations are structured in a way that allows the CPU to overlap execution and hide instruction latencies. In particular, the accumulation of acceleration components along each axis is performed using separate scalar variables, enabling the processor to execute these operations concurrently where possible.

Several code-level decisions further improve ILP. Particle position values for the outer-loop body are loaded once into local variables before entering the inner loop, reducing redundant memory accesses and allowing the compiler to better schedule dependent instructions. Additionally, the use of a structure-of-arrays data layout results in contiguous memory accesses, which improves cache behavior and allows the processor to issue multiple independent load instructions in parallel.

The removal of the conditional branch that excludes self-interactions eliminates a potential control dependency inside the inner loop. This reduces branch misprediction penalties and allows the instruction pipeline to remain fully utilized. As such, the inner loop consists primarily of straight-line code with predictable control flow, which is well suited for both static compiler scheduling and dynamic out-of-order execution. While these changes do not alter the algorithmic complexity, they improve single-core efficiency by increasing effective instruction throughput.

\subsection{Single Instruction, Multiple Data (SIMD)}

In addition to instruction-level parallelism, the implementation exposes opportunities for data-level parallelism through SIMD execution. The inner force computation loop operates on arrays of particle data and applies the same sequence of arithmetic operations to each element. This regular structure makes the loop a suitable candidate for vectorization, allowing multiple interactions to be processed simultaneously using wide vector registers.

The use of a structure-of-arrays data layout is particularly beneficial for SIMD execution. Particle positions, velocities, and masses are stored in separate contiguous arrays, enabling the compiler to generate vector load and store instructions with predictable memory access patterns. As such, multiple values of position or mass can be loaded into vector registers in a single instruction, improving throughput and reducing loop overhead.

SIMD parallelism is explicitly encouraged through the use of OpenMP \texttt{simd} directives on both the force accumulation loop and the state update loop. In the force computation, a reduction clause is used to safely accumulate partial results into scalar acceleration variables. This allows the compiler to vectorize the loop while preserving correctness, even though the accumulation introduces loop-carried dependencies at the scalar level. The removal of conditional branches within the loop further simplifies vectorization by maintaining a uniform control flow across iterations.

Similarly, the velocity and position update loop applies identical operations to independent elements of the state arrays and can be vectorized without requiring reductions. While SIMD execution does not change the algorithmic complexity of the simulation, it increases effective arithmetic throughput and improves single-core performance. As such, SIMD optimizations complement instruction-level parallelism and provide a more efficient foundation for thread-level parallelization in the shared-memory implementation.

\begin{lstlisting}[language=C++]
void PhysicsEngine::integrateStep(SoABodies& bodies, double dt) {
    const double G = 6.67430e-11;
    size_t n = bodies.size();

    // Reset accelerations
    std::fill(bodies.acc_x.begin(), bodies.acc_x.end(), 0.0);
    std::fill(bodies.acc_y.begin(), bodies.acc_y.end(), 0.0);
    std::fill(bodies.acc_z.begin(), bodies.acc_z.end(), 0.0);

    for (size_t i = 0; i < n; ++i) {
        double ax = 0.0, ay = 0.0, az = 0.0;

        double px = bodies.pos_x[i];
        double py = bodies.pos_y[i];
        double pz = bodies.pos_z[i];

        #pragma omp simd reduction(+:ax, ay, az)
        for (size_t j = 0; j < n; ++j) {
            // if (i == j) continue; removed to optimize performance
            
            double rx = bodies.pos_x[j] - px;
            double ry = bodies.pos_y[j] - py;
            double rz = bodies.pos_z[j] - pz;
            
            double distSq = rx * rx + ry * ry + rz * rz + 1e-9;
            double inv_dist = 1.0 / std::sqrt(distSq);
            double inv_dist3 = inv_dist * inv_dist * inv_dist;
            double factor = G * bodies.mass[j] * inv_dist3;
            ax += factor * rx;
            ay += factor * ry;
            az += factor * rz;
            
        }
        bodies.acc_x[i] += ax;
        bodies.acc_y[i] += ay;
        bodies.acc_z[i] += az;
    }

    #pragma omp simd
    for (size_t i = 0; i < n; ++i) {
        bodies.vel_x[i] += bodies.acc_x[i] * dt;
        bodies.vel_y[i] += bodies.acc_y[i] * dt;
        bodies.vel_z[i] += bodies.acc_z[i] * dt;
        
        bodies.pos_x[i] += bodies.vel_x[i] * dt;
        bodies.pos_y[i] += bodies.vel_y[i] * dt;
        bodies.pos_z[i] += bodies.vel_z[i] * dt;
    }
}
\end{lstlisting}


While SIMD vectorization improves single-core performance by exploiting data-level parallelism within individual loops, it does not address parallelism across multiple cores. Thread-level parallelization and SIMD execution therefore target different levels of the hardware execution model. As such, SIMD optimizations can be applied independently of the chosen threading strategy and serve to increase the amount of useful work performed per core. This combination allows both forms of parallelism to complement one another, improving overall scalability when the simulation is executed on multicore CPUs.



\section{Experimental Results}

This section presents the performance evaluation of the different optimization phases. The experiments were conducted on the SeARCH cluster, using an Intel-based architecture.

\subsection{Evaluation Metrics}

To rigorously evaluate the performance of the N-body simulation, a comprehensive set of metrics was collected to capture both the runtime behavior and the architectural efficiency of the code.

Execution Time statistics (Best, Mean, Median, Standard Deviation) provide a holistic view of performance. The \textbf{Best} (minimum) time is useful for estimating the peak performance capability of the code in the absence of external system interference (OS jitter). The \textbf{Mean} and \textbf{Median} offer a realistic expectation of performance in a shared environment, while the \textbf{Standard Deviation} indicates the stability and reproducibility of the measurements.

The architectural efficiency is measured through \textbf{Instructions Per Cycle (IPC)} and \textbf{L1 Cache Miss Rate}. IPC indicates the efficiency of the processor's instruction pipeline, where a higher IPC generally suggests better utilization of execution units and fewer stall cycles due to dependencies or memory latency. Conversely, the L1 Cache Miss Rate measures the effectiveness of the memory access patterns. A lower miss rate indicates better temporal and spatial locality, ensuring that the CPU spends less time waiting for data to be fetched from lower levels of the memory hierarchy.

\subsection{Performance Analysis}

Table \ref{tab:phase0} shows the baseline performance of the sequential brute-force implementation ($O(N^2)$). As expected, the execution time grows quadratically with the number of bodies $N$.

\begin{table}[h]
\centering
\caption{Phase 0: Sequential Baseline Results (SeARCH x86)}
\label{tab:phase0}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{N} & \textbf{Best (s)} & \textbf{Mean (s)} & \textbf{Median (s)} & \textbf{StdDev} & \textbf{IPC} & \textbf{L1 Miss \%} \\ \hline
1000 & 0.309 & 0.319 & 0.314 & 0.012 & 0.93 & 9.37\% \\ \hline
5000 & 5.908 & 5.958 & 5.921 & 0.089 & 0.80 & 14.74\% \\ \hline
10000 & 22.727 & 22.841 & 22.815 & 0.121 & 0.78 & 15.89\% \\ \hline
50000 & 552.436 & 552.618 & 552.618 & 0.257 & 0.76 & 16.94\% \\ \hline
\end{tabular}
\end{table}

Table \ref{tab:phase1} demonstrates the impact of changing the data layout from Array of Structures (AoS) to Structure of Arrays (SoA). This modification yields improved cache locality, indicated by the significant reduction in L1 cache miss rates (e.g., from 9.37\% to 0.66\% for N=1000).

\begin{table}[h]
\centering
\caption{Phase 1: Sequential Data Layout (AoS to SoA)}
\label{tab:phase1}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{N} & \textbf{Best (s)} & \textbf{Mean (s)} & \textbf{Median (s)} & \textbf{StdDev} & \textbf{IPC} & \textbf{L1 Miss \%} & \textbf{Speedup} \\ \hline
1000 & 0.303 & 0.314 & 0.306 & 0.015 & 0.85 & 0.66\% & 1.02 \\ \hline
5000 & 5.743 & 5.772 & 5.777 & 0.015 & 0.70 & 9.49\% & 1.03 \\ \hline
10000 & 22.181 & 22.232 & 22.242 & 0.036 & 0.67 & 10.81\% & 1.02 \\ \hline
\end{tabular}
\end{table}

Table \ref{tab:phase3} presents the results after enabling SIMD vectorization. By exploiting data-level parallelism, further speedups were achieved, maintaining efficiency even as the problem size scales.

\begin{table}[h]
\centering
\caption{Phase 3: Vectorization (SIMD / SVE)}
\label{tab:phase3}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{N} & \textbf{Best (s)} & \textbf{Mean (s)} & \textbf{Median (s)} & \textbf{StdDev} & \textbf{IPC} & \textbf{L1 Miss \%} & \textbf{Speedup} \\ \hline
1000 & 0.305 & 0.309 & 0.307 & 0.007 & 0.89 & 0.58\% & 1.01 \\ \hline
5000 & 5.797 & 5.832 & 5.810 & 0.058 & 0.75 & 6.10\% & 1.02 \\ \hline
10000 & 22.182 & 22.237 & 22.245 & 0.037 & 0.73 & 6.61\% & 1.02 \\ \hline
50000 & 539.574 & 540.916 & 540.638 & 1.373 & 0.71 & 8.77\% & 1.02 \\ \hline
\end{tabular}
\end{table}

Table \ref{tab:phase4} presents the results for shared memory parallelism using OpenMP. The experiments show significant speedups as the number of threads increases, utilizing the multicore architecture effectively. The results also demonstrate good scalability for larger problem sizes (N=100000 and N=500000), although diminishing returns are observed at higher thread counts due to synchronization overhead and memory bandwidth saturation.

\begin{table}[h]
\centering
\caption{Phase 4: Shared Memory Parallelism (OpenMP)}
\label{tab:phase4}
\begin{tabular}{|l|l|l|l|l|l|l|}
\hline
\textbf{N} & \textbf{Threads} & \textbf{Best (s)} & \textbf{Mean (s)} & \textbf{Speedup} & \textbf{IPC} & \textbf{L1 Miss \%} \\ \hline
50000 & 5 & 117.291 & 117.580 & 4.60 & 0.70 & 8.94\% \\ \hline
50000 & 10 & 55.857 & 56.109 & 9.66 & 0.70 & 9.15\% \\ \hline
50000 & 16 & 53.458 & 54.043 & 10.09 & 0.51 & 8.71\% \\ \hline
50000 & 24 & 48.032 & 48.399 & 11.23 & 0.37 & 8.46\% \\ \hline
50000 & 32 & 21.604 & 23.124 & 24.98 & 0.67 & 7.65\% \\ \hline
100000 & 5 & 460.053 & 460.887 & - & 0.70 & 9.10\% \\ \hline
100000 & 10 & 212.567 & 213.361 & - & 0.70 & 9.10\% \\ \hline
100000 & 16 & 198.754 & 201.962 & - & 0.50 & 8.86\% \\ \hline
100000 & 24 & 182.125 & 183.011 & - & 0.37 & 8.64\% \\ \hline
100000 & 32 & 68.013 & 69.930 & - & 0.67 & 7.89\% \\ \hline
500000 & 16 & 1617.962 & 1617.962 & - & - & - \\ \hline

\end{tabular}
\end{table}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.8\linewidth]{figures/time_plot.png}
  \caption{Execution Time vs Number of Threads (Log Scale)}
  \Description{Execution Time vs Number of Threads (Log Scale)}
  \label{fig:time_plot}
\end{figure}

\bibliographystyle{plain}
\bibliography{refs.bib}

\end{document}
\endinput
